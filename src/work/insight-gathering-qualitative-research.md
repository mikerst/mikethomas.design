---
title: Gathering insight through qualitative research
intro: A short research study aimed at helping to inform how a key customer touchpoint might be improved.
hero: case-study__research__hero.jpg
date: 2018-12-14
tags: 
- ux
theme: 
---

## Background

Throughout 2018 TotallyMoney made ~£1.2 million in gross profit from sending product alert emails to customers—who'd opted-in—providing them with the latest relevent offers from the credit cards and loans markets. However, during this time the content, structure and design had remained largely unchanged.

There was a desire within the business to update and refresh the design of our alerts, to make them even better for customers as well as more performative from a revenue point of view. We had a lot of quantitative data we could look at (open rates, click through rates, et cetera) to help inform any design decisions we might make, however we lacked anything qualitative. In order to better understand what our customers thought about our product alert emails, we chose to conduct a brief research study to see if we could uncover some useful insight.


## Why re-design?

We knew we'd not made any significant changes since we first introduced the touchpoint. From a performance marketing and business revenue perspective, everything was going well, but some nagging questions always remained in the background. With the right approach, how might we unlock even better performance _and_ improve customer satisfaction? 

Instinctively, we felt we could do better.

<figure>
	<img src="/_assets/img/case-study__research__alerts.jpg" />
	<figcaption>Small sample of alerts across a 12 month period with only minor evolution.</figcaption>
</figure>

## My role

With the support and experience of TotallyMoney's UX lead, I worked to plan, conduct and analyse a short research study that aimed to help TotallyMoney better understand what customers thought of product alert emails being sent out on a regular basis. 


## Objectives

We started by setting some primary and secondary objectives. We knew from our data that approximately 25% of customers who opened these emails would click through and the remainder would not. Digging in to _why_ this might be the case seemed to be a sensible initial area of focus. 

### Primary

_We want to re-design our product alert emails, but need to do so from an informed position. To help us identify <mark>where we should prioritise our efforts</mark>, we should speak to our customers in order to <mark>establish why</mark> some customers click out and apply for credit from our product alert emails, and why some do not complete this journey._

The above is nice and broad, and sets the general direction of travel—the orientation—for a more detailed set of secondary objectives.

### Secondary

I won't list them all—a copy of the original planning document [can be viewed here](https://paper.dropbox.com/doc/Research-Plan-Product-Alert-Redesign-UnmoderatedRemote--AiXKsqfi60kJdDywTdQdKB6OAQ-tZXs9XmqXSKl1mnOXMPK4)—however from a high-level point of view, secondary objectives were split in to various themes of inquiry:

E.g. 

_Content_, _visual design_, _customer expectations_

Within these themes, we could pose various questions that we'd like to get some answers to:

E.g. 

_How does the customer feel about the tone of voice used in the copy?_

_What is the most important information in the email?_

_How often would the customer expect to receive such an email_?

## Sourcing the right candidates

As this was to be a short initial study, we opted to conduct the research in a remote, unmoderated context via UserTesting.com, that provide some useful built-in tools that allowed us to source the best possible candidates.

### Demographic

For security and data protection reasons we weren't in a position to make use of our existing customer database, but we wanted to ensure that candidates matched, or closely resembled customers in one of TotallyMoney's key customer segments. The following would therefore needed to be true:

1. To ensure market familiarity — candidates must have engaged in the credit marketing in the preceeding 12 months in some form. 
2. We knew that our traffic from this email was very heavily skewed towards mobile users at approximately 80%, and so we wanted candidates with a similar likelyhood of being on a mobile device.
3. Candidates must be from the UK.
4. Candidates should ideally be in the £10—£25k income bracket. 


### Screener

UserTesting.com allows some of the criteria mentioned above to be applied as a filter. Beyond that, we screened candidates by posing some initial questions up front. 

When doing this it was important to set the questions up correctly. It's important to ensure that only the most appropriate, genuine candidates pass the screener. In this instance it was particularly important as it was our intention only to run the study with a small group. 

<figure>
	<img src="/_assets/img/case-study__research__screener.jpg" />
	<figcaption>Getting the screener right helps to improve the chances of getting the right candidates.</figcaption>
</figure>

## Prototype

One of the more unusual aspects of this study was that we'd be using an email touchpoint.

We wanted candidates to experience the email in a setting as close akin to an inbox, but actually sending out an email wasn't possible. Furthermoore, wanted candidates to experience the demo email in a way that was responsive to their device size. We opted to send candidates through to a browser-based demo email, that acted as the closest match we could get to proving a real inbox experience in terms of visible area and responsive behaviour. 

<figure>
	<img src="/_assets/img/case-study__research__proto.jpg" />
	<figcaption>Using the 'view online' version of the email on mobile as a prototype.</figcaption>
</figure>

## Script

As this was to be an unmoderated, remote study, candidates would be interacting directly with a set of tasks via the UserTesting.com app. The script, or wording for each question made varied used of UserTesting's range of question options, focusing largely on simple prompts to get the candidate talking mixed in with some rating scale questions in an attempt to provide some basic data points we could look at and refer to. 

It was important to us to respect the time of our candidates and so we wanted the study to be light on hoops to jump through, albeit with space to allow candidates to elaborate if they wanted to.

A draft version of the script [can be viewed in the planning document](https://paper.dropbox.com/doc/Research-Plan-Product-Alert-Redesign-UnmoderatedRemote--AiXKsqfi60kJdDywTdQdKB6OAQ-tZXs9XmqXSKl1mnOXMPK4) mentioned previously. 


<figure>
	<img src="/_assets/img/case-study__research__script.jpg" />
	<figcaption>Utilising different types of questions breaks up the study, and allows for different kinds of responses and insight.</figcaption>
</figure>


## Analysis

Once everything was ready to go, we launched a pilot of the study to see if there was anything we needed to adjust. Once any adjustments were made, we launched the study with an additional four candidates, bringing us up to five, which we eventually extended to ten candidates. Once we had our ten, we could play back each study abd begin the process of analysing what we'd found.

### Wide view

UserTesting.com allows for videos to be watched back and annotated in real-time. However once all of the notes and comments were added, they were transcribed in to a Google Sheet with questions in the Y axis and users in the X. This proved to be a great way to quickly scan through each area of interest for us, enabling quick identification of key themes and patterns.

<figure>
	<img src="/_assets/img/case-study__research__findings.jpg" />
	<figcaption>Using Google Sheets was a great way to capture findings. Usernames redacted out of courtesy!</figcaption>
</figure>

### Panning for gold

Starting with the wide view, we were able sift through the findings and create an initial loose analysis that communicated key takeaways. These were written up and matched against each section of the study, along with links to a highlights reel of key moments as spoken by the candidates as corroboration. 

### Recommendations

## Lessons learned